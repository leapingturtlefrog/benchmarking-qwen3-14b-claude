LLM Stats Logo
llm-stats.com
Leaderboards
Benchmarks

Compare
Arenas

News
Gateway

Search
⌘
K

Sign in
Toggle theme
Benchmarks
math
AIME 2025
AIME 2025
All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.

Paper
DatasetSoon
CodeSoon

Details

Discussions
0

Reviews
0
Progress Over Time
Interactive timeline showing model performance evolution on AIME 2025



State-of-the-art frontier
Open
Proprietary
AIME 2025 Leaderboard
All
Open
Proprietary
100 models • 0 verified
Rank	Model	Score	Size	Context	Cost	License
1
OpenAI
GPT-5.2 Pro
OpenAI
1.000	—	400K	
$21.00
$168.00
1
Moonshot AI
Kimi K2-Thinking-0905
Moonshot AI
1.000	1.0T	262K	
$0.47
$2.00
1
xAI
Grok-4 Heavy
xAI
1.000	—	—	—	
1
Google
Gemini 3 Pro
Google
1.000	—	1.0M	
$2.00
$12.00
1
OpenAI
GPT-5.2
OpenAI
1.000	—	400K	
$1.75
$14.00
6
Anthropic
Claude Opus 4.6
Anthropic
0.998	—	200K	
$5.00
$25.00
7
Google
Gemini 3 Flash
Google
0.997	—	1.0M	
$0.50
$3.00
8
Meituan
LongCat-Flash-Thinking-2601
Meituan
0.996	560B	128K	
$0.30
$1.20
8
OpenAI
GPT-5.1 High
OpenAI
0.996	—	400K	
$1.25
$10.00
10
NVIDIA
Nemotron 3 Nano (30B A3B)
NVIDIA
0.992	32B	262K	
$0.06
$0.24
11
OpenAI
GPT OSS 20B High
OpenAI
0.987	21B	131K	
$0.10
$0.50
12
OpenAI
GPT-5.1 Medium
OpenAI
0.984	—	400K	
$1.25
$10.00
13
ByteDance
Seed 2.0 Pro
ByteDance
0.983	—	—	—	
14
StepFun
Step-3.5-Flash
StepFun
0.973	196B	66K	
$0.10
$0.40
15
OpenAI
GPT-5.1 Codex High
OpenAI
0.967	—	400K	
$1.25
$10.00
16
Moonshot AI
Kimi K2.5
Moonshot AI
0.961	1.0T	262K	
$0.60
$2.50
17
DeepSeek
DeepSeek-V3.2-Speciale
DeepSeek
0.960	685B	131K	
$0.28
$0.42
18
Zhipu AI
GLM-4.7
Zhipu AI
0.957	358B	205K	
$0.60
$2.20
19
OpenAI
GPT-5
OpenAI
0.946	—	400K	
$1.25
$10.00
19
OpenAI
GPT-5 High
OpenAI
0.946	—	400K	
$1.25
$10.00
21
Xiaomi
MiMo-V2-Flash
Xiaomi
0.941	309B	256K	
$0.10
$0.30
22
OpenAI
GPT-5.1 Thinking
OpenAI
0.940	—	400K	
$1.25
$10.00
22
OpenAI
GPT-5.1
OpenAI
0.940	—	400K	
$1.25
$10.00
22
OpenAI
GPT-5.1 Instant
OpenAI
0.940	—	400K	
$1.25
$10.00
25
Zhipu AI
GLM-4.6
Zhipu AI
0.939	357B	131K	
$0.55
$2.19
26
xAI
Grok-3
xAI
0.933	—	128K	
$3.00
$15.00
27
DeepSeek
DeepSeek-V3.2 (Thinking)
DeepSeek
0.931	685B	131K	
$0.28
$0.42
28
ByteDance
Seed 2.0 Lite
ByteDance
0.930	—	—	—	
29
LG AI Research
K-EXAONE-236B-A23B
LG AI Research
0.928	236B	33K	
$0.60
$1.00
30
OpenAI
o4-mini
OpenAI
0.927	—	200K	
$1.10
$4.40
31
OpenAI
GPT OSS 120B High
OpenAI
0.925	117B	131K	
$0.10
$0.50
32
Alibaba Cloud / Qwen Team
Qwen3-235B-A22B-Thinking-2507
Alibaba Cloud / Qwen Team
0.923	235B	262K	
$0.30
$3.00
33
xAI
Grok 4 Fast
xAI
0.920	—	2.0M	
$0.20
$0.50
34
xAI
Grok-4
xAI
0.917	—	256K	
$3.00
$15.00
35
Zhipu AI
GLM-4.7-Flash
Zhipu AI
0.916	30B	128K	
$0.07
$0.40
36
OpenAI
GPT-5 mini
OpenAI
0.911	—	400K	
$0.25
$2.00
37
xAI
Grok-3 Mini
xAI
0.908	—	128K	
$0.30
$0.50
38
Meituan
LongCat-Flash-Thinking
Meituan
0.906	560B	128K	
$0.30
$1.20
39
Alibaba Cloud / Qwen Team
Qwen3 VL 235B A22B Thinking
Alibaba Cloud / Qwen Team
0.897	236B	262K	
$0.45
$3.49
40
DeepSeek
DeepSeek-V3.2-Exp
DeepSeek
0.893	685B	164K	
$0.27
$0.41
41
OpenAI
GPT-5 Medium
OpenAI
0.889	—	400K	
$1.25
$10.00
42
Google
Gemini 2.5 Pro Preview 06-05
Google
0.880	—	1.0M	
$1.25
$10.00
43
Alibaba Cloud / Qwen Team
Qwen3-Next-80B-A3B-Thinking
Alibaba Cloud / Qwen Team
0.878	80B	66K	
$0.15
$1.50
44
StepFun
Step3-VL-10B
StepFun
0.877	10B	—	—	
45
DeepSeek
DeepSeek-R1-0528
DeepSeek
0.875	671B	131K	
$0.50
$2.15
46
Baidu
ERNIE 5.0
Baidu
0.870	—	—	—	
46
Anthropic
Claude Sonnet 4.5
Anthropic
0.870	—	200K	
$3.00
$15.00
48
OpenAI
o3
OpenAI
0.864	—	200K	
$2.00
$8.00
49
OpenAI
GPT-5 nano
OpenAI
0.852	—	400K	
$0.05
$0.40
50
Mistral AI
Ministral 3 (14B Reasoning 2512)
Mistral AI
0.850	14B	262K	
$0.20
$0.20
Showing 1-50 of 100
Previous
1 / 2
Next
Notice missing or incorrect data?
Start an Issue discussion
→
FAQ
Common questions about AIME 2025


What is the AIME 2025 benchmark?
All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.

Where can I find the AIME 2025 paper?

What is the AIME 2025 leaderboard?

What is the highest AIME 2025 score?

How many models are evaluated on AIME 2025?

What categories does AIME 2025 cover?
Statistics
Total Models
100
Average Score
0.776
Best Score
1.000
Std Deviation
0.220
Properties
Categories
math
reasoning
Modality
text
Language
EN
Max Score
1
Verification
Verified
0
Self-reported
100
Status
Unverified
Join our newsletter and stay up to date with everything AI
There's too much noise in AI, let's filter it for you. Get a curated digest of models, benchmarks, and the analysis that matters, right in your inbox once a week.

you@example.com

Subscribe
No spam, unsubscribe anytime

LLM Stats Logo
llm-stats.com
The AI Benchmarking Hub.

Leaderboards
AI Leaderboards
LLM Leaderboard
Open LLM Leaderboard
Best AI for Coding
Best AI for Math
Best AI for Image Generation
Best AI for Writing
Arenas
All Arenas
Chat Arena
Coding Arena
Image Arena
Video Arena
Audio Arena
Trading Arena
AI Image Generator
AI Photo Editor
Benchmarks
GPQA
MMLU
MMLU-Pro
AIME 2025
MATH
HumanEval
MMMU
LiveCodeBench
IFEval
GSM8K
SWE-Bench Verified
Models
Gemini 3 Pro
Grok-4 Heavy
GPT-5.1
Grok-4
Qwen3-235B-A22B-Thinking
DeepSeek-R1-0528
GLM-4.6
GPT OSS 120B
Resources
Playground
Blog
News
Community
API
Infrastructure
© 2026 llm-stats

About us
Privacy policy
Terms of service
AIME 2025 Leaderboard


LLM Stats Logo
llm-stats.com
Leaderboards
Benchmarks

Compare
Arenas

News
Gateway

Search
⌘
K

Sign in
Toggle theme
Benchmarks
math
AIME 2025
AIME 2025
All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.

Paper
DatasetSoon
CodeSoon

Details

Discussions
0

Reviews
0
Progress Over Time
Interactive timeline showing model performance evolution on AIME 2025



State-of-the-art frontier
Open
Proprietary
AIME 2025 Leaderboard
All
Open
Proprietary
100 models • 0 verified
Rank	Model	Score	Size	Context	Cost	License
51
Alibaba Cloud / Qwen Team
Qwen3 VL 32B Thinking
Alibaba Cloud / Qwen Team
0.837	33B	—	—	
52
Alibaba Cloud / Qwen Team
Qwen3 VL 30B A3B Thinking
Alibaba Cloud / Qwen Team
0.831	31B	262K	
$0.20
$1.00
53
Google
Gemini 2.5 Pro
Google
0.830	—	1.0M	
$1.25
$10.00
54
Alibaba Cloud / Qwen Team
Qwen3 Max
Alibaba Cloud / Qwen Team
0.816	1.0T	256K	
$0.50
$5.00
55
Alibaba Cloud / Qwen Team
Qwen3 235B A22B
Alibaba Cloud / Qwen Team
0.815	235B	128K	
$0.10
$0.10
56
MiniMax
MiniMax M2.1
MiniMax
0.810	230B	1.0M	
$0.30
$1.20
57
Anthropic
Claude Haiku 4.5
Anthropic
0.807	—	200K	
$1.00
$5.00
58
Alibaba Cloud / Qwen Team
Qwen3 VL 8B Thinking
Alibaba Cloud / Qwen Team
0.803	9B	262K	
$0.18
$2.09
59
Mistral AI
Ministral 3 (8B Reasoning 2512)
Mistral AI
0.787	8B	262K	
$0.15
$0.15
60
Microsoft
Phi 4 Reasoning Plus
Microsoft
0.780	14B	—	—	
60
Anthropic
Claude Opus 4.1
Anthropic
0.780	—	200K	
$15.00
$75.00
60
MiniMax
MiniMax M2
MiniMax
0.780	230B	1.0M	
$0.30
$1.20
63
MiniMax
MiniMax M1 80K
MiniMax
0.769	456B	1.0M	
$0.55
$2.20
64
Anthropic
Claude Opus 4
Anthropic
0.755	—	200K	
$15.00
$75.00
65
Alibaba Cloud / Qwen Team
Qwen3 VL 235B A22B Instruct
Alibaba Cloud / Qwen Team
0.747	236B	262K	
$0.30
$1.49
66
MiniMax
MiniMax M1 40K
MiniMax
0.746	456B	—	—	
67
Alibaba Cloud / Qwen Team
Qwen3 VL 4B Thinking
Alibaba Cloud / Qwen Team
0.745	4B	262K	
$0.10
$1.00
68
Alibaba Cloud / Qwen Team
Qwen3 32B
Alibaba Cloud / Qwen Team
0.729	33B	128K	
$0.10
$0.44
69
NVIDIA
Llama 3.1 Nemotron Ultra 253B v1
NVIDIA
0.725	253B	—	—	
70
Mistral AI
Min istral 3 (3B Reasoning 2512)
Mistral AI
0.721	3B	131K	
$0.10
$0.10
70
NVIDIA
Nemotron Nano 9B v2
NVIDIA
0.721	9B	—	—	
72
Google
Gemini 2.5 Flash
Google
0.720	—	1.0M	
$0.30
$2.50
73
Alibaba Cloud / Qwen Team
Qwen3 30B A3B
Alibaba Cloud / Qwen Team
0.709	31B	128K	
$0.10
$0.44
74
Anthropic
Claude Sonnet 4
Anthropic
0.705	—	200K	
$3.00
$15.00
75
Alibaba Cloud / Qwen Team
Qwen3-235B-A22B-Instruct-2507
Alibaba Cloud / Qwen Team
0.703	235B	262K	
$0.15
$0.80
76
Alibaba Cloud / Qwen Team
Qwen3-Next-80B-A3B-Instruct
Alibaba Cloud / Qwen Team
0.695	80B	66K	
$0.15
$1.50
77
Alibaba Cloud / Qwen Team
Qwen3 VL 30B A3B Instruct
Alibaba Cloud / Qwen Team
0.693	31B	262K	
$0.20
$0.70
78
Alibaba Cloud / Qwen Team
Qwen3 VL 32B Instruct
Alibaba Cloud / Qwen Team
0.662	33B	—	—	
79
Mistral AI
Magistral Medium
Mistral AI
0.649	24B	—	—	
80
Meituan
LongCat-Flash-Lite
Meituan
0.632	69B	256K	
$0.10
$0.40
81
Microsoft
Phi 4 Reasoning
Microsoft
0.629	14B	—	—	
82
Mistral AI
Magistral Small 2506
Mistral AI
0.628	24B	—	—	
83
Meituan
LongCat-Flash-Chat
Meituan
0.613	560B	128K	
$0.30
$1.20
84
NVIDIA
Llama-3.3 Nemotron Super 49B v1
NVIDIA
0.584	50B	—	—	
85
Anthropic
Claude 3.7 Sonnet
Anthropic
0.548	—	200K	
$3.00
$15.00
86
Google
Gemini 2.5 Flash-Lite
Google
0.498	—	1.0M	
$0.10
$0.40
86
DeepSeek
DeepSeek-V3.1
DeepSeek
0.498	671B	164K	
$0.27
$1.00
88
Moonshot AI
Kimi K2 Instruct
Moonshot AI
0.495	1.0T	200K	
$0.50
$0.50
88
Moonshot AI
Kimi K2-Instruct-0905
Moonshot AI
0.495	1.0T	—	—	
90
NVIDIA
Llama 3.1 Nemotron Nano 8B V1
NVIDIA
0.471	8B	—	—	
91
Alibaba Cloud / Qwen Team
Qwen3 VL 4B Instruct
Alibaba Cloud / Qwen Team
0.466	4B	262K	
$0.10
$0.60
92
OpenAI
GPT-4.1
OpenAI
0.464	—	1.0M	
$2.00
$8.00
93
Alibaba Cloud / Qwen Team
Qwen3 VL 8B Instruct
Alibaba Cloud / Qwen Team
0.459	9B	262K	
$0.08
$0.50
94
OpenAI
GPT-5.1 Codex Mini
OpenAI
0.421	—	400K	
$0.25
$2.00
95
OpenAI
GPT-4.1 mini
OpenAI
0.402	—	1.0M	
$0.40
$1.60
96
Google
Gemini Diffusion
Google
0.233	—	—	—	
97
Google
Gemma 3n E4B Instructed
Google
0.116	8B	32K	
$20.00
$40.00
97
Google
Gemma 3n E4B Instructed LiteRT Preview
Google
0.116	2B	—	—	
99
Google
Gemma 3n E2B Instructed
Google
0.067	8B	—	—	
99
Google
Gemma 3n E2B Instructed LiteRT (Preview)
Google
0.067	2B	—	—	
Showing 51-100 of 100
Previous
2 / 2
Next
Notice missing or incorrect data?
Start an Issue discussion
→
FAQ
Common questions about AIME 2025


What is the AIME 2025 benchmark?
All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.

Where can I find the AIME 2025 paper?

What is the AIME 2025 leaderboard?

What is the highest AIME 2025 score?

How many models are evaluated on AIME 2025?

What categories does AIME 2025 cover?
Statistics
Total Models
100
Average Score
0.776
Best Score
1.000
Std Deviation
0.220
Properties
Categories
math
reasoning
Modality
text
Language
EN
Max Score
1
Verification
Verified
0
Self-reported
100
Status
Unverified
Join our newsletter and stay up to date with everything AI
There's too much noise in AI, let's filter it for you. Get a curated digest of models, benchmarks, and the analysis that matters, right in your inbox once a week.

you@example.com

Subscribe
No spam, unsubscribe anytime

LLM Stats Logo
llm-stats.com
The AI Benchmarking Hub.

Leaderboards
AI Leaderboards
LLM Leaderboard
Open LLM Leaderboard
Best AI for Coding
Best AI for Math
Best AI for Image Generation
Best AI for Writing
Arenas
All Arenas
Chat Arena
Coding Arena
Image Arena
Video Arena
Audio Arena
Trading Arena
AI Image Generator
AI Photo Editor
Benchmarks
GPQA
MMLU
MMLU-Pro
AIME 2025
MATH
HumanEval
MMMU
LiveCodeBench
IFEval
GSM8K
SWE-Bench Verified
Models
Gemini 3 Pro
Grok-4 Heavy
GPT-5.1
Grok-4
Qwen3-235B-A22B-Thinking
DeepSeek-R1-0528
GLM-4.6
GPT OSS 120B
Resources
Playground
Blog
News
Community
API
Infrastructure
© 2026 llm-stats

About us
Privacy policy
Terms of service
AIME 2025 Leaderboard