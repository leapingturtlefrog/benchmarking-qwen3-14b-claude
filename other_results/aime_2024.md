LLM Stats Logo
llm-stats.com
Leaderboards
Benchmarks

Compare
Arenas

News
Gateway

Search
⌘
K

Sign in
Toggle theme
Benchmarks
math
AIME 2024
AIME 2024
American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.

Paper
DatasetSoon
CodeSoon

Details

Discussions
0

Reviews
0
Progress Over Time
Interactive timeline showing model performance evolution on AIME 2024



State-of-the-art frontier
Open
Proprietary
AIME 2024 Leaderboard
All
Open
Proprietary
52 models • 0 verified
Rank	Model	Score	Size	Context	Cost	License
1
xAI
Grok-3 Mini
xAI
0.958	—	128K	
$0.30
$0.50
2
OpenAI
o4-mini
OpenAI
0.934	—	200K	
$1.10
$4.40
3
xAI
Grok-3
xAI
0.933	—	128K	
$3.00
$15.00
3
Meituan
LongCat-Flash-Thinking
Meituan
0.933	560B	128K	
$0.30
$1.20
5
Google
Gemini 2.5 Pro
Google
0.920	—	1.0M	
$1.25
$10.00
6
OpenAI
o3
OpenAI
0.916	—	200K	
$2.00
$8.00
7
DeepSeek
DeepSeek-R1-0528
DeepSeek
0.914	671B	131K	
$0.50
$2.15
8
Zhipu AI
GLM-4.5
Zhipu AI
0.910	355B	131K	
$0.40
$1.60
9
Mistral AI
Ministral 3 (14B Reasoning 2512)
Mistral AI
0.898	14B	262K	
$0.20
$0.20
10
Zhipu AI
GLM-4.5-Air
Zhipu AI
0.894	106B	—	—	
11
Google
Gemini 2.5 Flash
Google
0.880	—	1.0M	
$0.30
$2.50
12
OpenAI
o3-mini
OpenAI
0.873	—	200K	
$1.10
$4.40
13
DeepSeek
DeepSeek R1 Zero
DeepSeek
0.867	671B	—	—	
13
DeepSeek
DeepSeek R1 Distill Llama 70B
DeepSeek
0.867	71B	128K	
$0.10
$0.40
15
Mistral AI
Ministral 3 (8B Reasoning 2512)
Mistral AI
0.860	8B	262K	
$0.15
$0.15
15
MiniMax
MiniMax M1 80K
MiniMax
0.860	456B	1.0M	
$0.55
$2.20
15
OpenAI
o1-pro
OpenAI
0.860	—	—	—	
18
Alibaba Cloud / Qwen Team
Qwen3 235B A22B
Alibaba Cloud / Qwen Team
0.857	235B	128K	
$0.10
$0.10
19
MiniMax
MiniMax M1 40K
MiniMax
0.833	456B	—	—	
19
DeepSeek
DeepSeek R1 Distill Qwen 7B
DeepSeek
0.833	8B	—	—	
19
DeepSeek
DeepSeek R1 Distill Qwen 32B
DeepSeek
0.833	33B	128K	
$0.12
$0.18
22
Alibaba Cloud / Qwen Team
Qwen3 32B
Alibaba Cloud / Qwen Team
0.814	33B	128K	
$0.10
$0.44
23
Microsoft
Phi 4 Reasoning Plus
Microsoft
0.813	14B	—	—	
24
IBM
Granite 3.3 8B Instruct
IBM
0.812	8B	128K	
$0.50
$0.50
24
IBM
Granite 3.3 8B Base
IBM
0.812	8B	—	—	
26
Alibaba Cloud / Qwen Team
Qwen3 30B A3B
Alibaba Cloud / Qwen Team
0.804	31B	128K	
$0.10
$0.44
27
DeepSeek
DeepSeek R1 Distill Llama 8B
DeepSeek
0.800	8B	—	—	
27
DeepSeek
DeepSeek R1 Distill Qwen 14B
DeepSeek
0.800	15B	—	—	
27
Anthropic
Claude 3.7 Sonnet
Anthropic
0.800	—	200K	
$3.00
$15.00
30
Alibaba Cloud / Qwen Team
QwQ-32B
Alibaba Cloud / Qwen Team
0.795	33B	—	—	
31
Moonshot AI
Kimi-k1.5
Moonshot AI
0.775	—	—	—	
31
Mistral AI
Min istral 3 (3B Reasoning 2512)
Mistral AI
0.775	3B	131K	
$0.10
$0.10
33
Microsoft
Phi 4 Reasoning
Microsoft
0.753	14B	—	—	
34
OpenAI
o1
OpenAI
0.743	—	200K	
$15.00
$60.00
35
Mistral AI
Magistral Medium
Mistral AI
0.736	24B	—	—	
36
Google
Gemini 2.0 Flash Thinking
Google
0.733	—	—	—	
37
Meituan
LongCat-Flash-Lite
Meituan
0.722	69B	256K	
$0.10
$0.40
38
Moonshot AI
Kimi K2 0905
Moonshot AI
0.720	1.0T	262K	
$0.60
$2.50
39
Mistral AI
Magistral Small 2506
Mistral AI
0.707	24B	—	—	
40
Moonshot AI
Kimi K2 Instruct
Moonshot AI
0.696	1.0T	200K	
$0.50
$0.50
40
Moonshot AI
Kimi K2-Instruct-0905
Moonshot AI
0.696	1.0T	—	—	
42
DeepSeek
DeepSeek-V3.1
DeepSeek
0.663	671B	164K	
$0.27
$1.00
43
DeepSeek
DeepSeek-V3 0324
DeepSeek
0.594	671B	164K	
$0.28
$1.14
44
DeepSeek
DeepSeek R1 Distill Qwen 1.5B
DeepSeek
0.527	2B	—	—	
45
Alibaba Cloud / Qwen Team
QwQ-32B-Preview
Alibaba Cloud / Qwen Team
0.500	33B	33K	
$0.15
$0.60
46
OpenAI
GPT-4.1 mini
OpenAI
0.496	—	1.0M	
$0.40
$1.60
47
OpenAI
GPT-4.1
OpenAI
0.481	—	1.0M	
$2.00
$8.00
48
OpenAI
o1-preview
OpenAI
0.420	—	128K	
$15.00
$60.00
49
DeepSeek
DeepSeek-V3
DeepSeek
0.392	671B	131K	
$0.27
$1.10
50
OpenAI
GPT-4.5
OpenAI
0.367	—	128K	
$75.00
$150.00
Showing 1-50 of 52
Previous
1 / 2
Next
Notice missing or incorrect data?
Start an Issue discussion
→
FAQ
Common questions about AIME 2024


What is the AIME 2024 benchmark?
American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.

Where can I find the AIME 2024 paper?

What is the AIME 2024 leaderboard?

What is the highest AIME 2024 score?

How many models are evaluated on AIME 2024?

What categories does AIME 2024 cover?
Statistics
Total Models
52
Average Score
0.746
Best Score
0.958
Std Deviation
0.184
Properties
Categories
math
reasoning
Modality
text
Language
EN
Max Score
1
Verification
Verified
0
Self-reported
52
Status
Unverified
Join our newsletter and stay up to date with everything AI
There's too much noise in AI, let's filter it for you. Get a curated digest of models, benchmarks, and the analysis that matters, right in your inbox once a week.

you@example.com

Subscribe
No spam, unsubscribe anytime

LLM Stats Logo
llm-stats.com
The AI Benchmarking Hub.

Leaderboards
AI Leaderboards
LLM Leaderboard
Open LLM Leaderboard
Best AI for Coding
Best AI for Math
Best AI for Image Generation
Best AI for Writing
Arenas
All Arenas
Chat Arena
Coding Arena
Image Arena
Video Arena
Audio Arena
Trading Arena
AI Image Generator
AI Photo Editor
Benchmarks
GPQA
MMLU
MMLU-Pro
AIME 2025
MATH
HumanEval
MMMU
LiveCodeBench
IFEval
GSM8K
SWE-Bench Verified
Models
Gemini 3 Pro
Grok-4 Heavy
GPT-5.1
Grok-4
Qwen3-235B-A22B-Thinking
DeepSeek-R1-0528
GLM-4.6
GPT OSS 120B
Resources
Playground
Blog
News
Community
API
Infrastructure
© 2026 llm-stats

About us
Privacy policy
Terms of service
AIME 2024 Leaderboard