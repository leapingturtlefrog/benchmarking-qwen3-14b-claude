# Final Report: Distillation Evaluation of Qwen3-14B with Claude 4.5 Opus Reasoning

**A comprehensive evaluation of whether distilling chain-of-thought reasoning from Claude 4.5 Opus into Qwen3-14B improves or degrades performance across math, coding, and instruction-following benchmarks.**

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Methodology](#2-methodology)
3. [Overall Scorecard](#3-overall-scorecard)
4. [Statistical Significance](#4-statistical-significance)
5. [AIME Results: Math Reasoning Degraded](#5-aime-results-math-reasoning-degraded)
6. [IFEval Results: Instruction Following Improved](#6-ifeval-results-instruction-following-improved)
7. [HumanEval+ Results: Coding Dramatically Improved](#7-humaneval-results-coding-dramatically-improved)
8. [Reasoning Volume Analysis (Proxy Logs)](#8-reasoning-volume-analysis-proxy-logs)
9. [Training Dataset Composition](#9-training-dataset-composition)
10. [Dataset vs Performance: What Drove the Results](#10-dataset-vs-performance-what-drove-the-results)
11. [Comparison with Other Models](#11-comparison-with-other-models)
12. [Conclusions](#12-conclusions)

---

## 1. Executive Summary

We fine-tuned Qwen3-14B on 250 chain-of-thought examples generated by Claude 4.5 Opus, producing `TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF`. We then evaluated both models on four benchmarks: AIME 2024, AIME 2025, IFEval, and HumanEval+.

### The Headline Results

| Benchmark | Distilled | Base Qwen3-14B | Change | Significant? |
|---|---|---|---|---|
| **HumanEval+** (coding) | **83.5%** (137/164) | 55.5% (91/164) | **+28.0pp** üöÄ | p = 4.37√ó10‚Åª¬π‚Å∞ (***) |
| **IFEval** (instruction following) | **77.3%** (418/541) | 63.6% (344/541) | **+13.7pp** ‚úÖ | p = 3.13√ó10‚Åª‚Å∏ (***) |
| **AIME 2024** (competition math) | 56.7% (17/30) | **70.0%** (21/30) | **-13.3pp** üìâ | p = 0.344 (ns) |
| **AIME 2025** (competition math) | 43.3% (13/30) | **66.7%** (20/30) | **-23.4pp** üìâ | p = 0.039 (*) |
| **AIME Combined** (60 questions) | 50.0% (30/60) | **68.3%** (41/60) | **-18.3pp** üìâ | p = 0.019 (*) |

### The Story in One Paragraph

Distilling 250 chain-of-thought examples from Claude 4.5 Opus into Qwen3-14B produced **massive gains on coding (+28pp) and instruction following (+13.7pp)**, but **statistically significant degradation on competition math (-18.3pp combined)**. The distilled model learned Claude's structured reasoning style ‚Äî it adds `<think>` blocks to 46% of coding responses and produces longer, more thorough completions ‚Äî but it reasons **3√ó less** on math problems than the base model. More reasoning correlates with correctness for the base model, but the distilled model's compressed reasoning chains aren't sufficient for competition-level mathematics. The training dataset's math content (simple arithmetic) was too easy to transfer to AIME-level problems, while its general chain-of-thought patterns transferred powerfully to coding and instruction following.

---

## 2. Methodology

### 2.1 Models

| Model | Description | Serving |
|---|---|---|
| **Distilled** | `TeichAI/Qwen3-14B-Claude-4.5-Opus-High-Reasoning-Distill-GGUF` | llama.cpp, port 8766 |
| **Base** | `Qwen/Qwen3-14B-GGUF` | llama.cpp, port 8767 |

### 2.2 Benchmarks

| Benchmark | Questions | What It Tests |
|---|---|---|
| AIME 2024 | 30 | Competition-level mathematics (number theory, geometry, combinatorics) |
| AIME 2025 | 30 | Competition-level mathematics (same format, newer problems) |
| IFEval | 541 | Precise instruction following (format constraints, word counts, etc.) |
| HumanEval+ | 164 | Code correctness (function implementation with test cases) |

### 2.3 Evaluation Framework

- **Framework**: `lm-evaluation-harness` v0.4.11 with OpenAI-compatible chat completions
- **Proxy logging**: All API requests were routed through a logging proxy to capture full request/response data, including `<think>` reasoning blocks
- **Statistical tests**: McNemar's test (paired comparison), Wilson confidence intervals, Cohen's h effect sizes
- **Caching**: Results cached to `.cache/` to ensure reproducibility; some runs served from cache (noted in match rates)

### 2.4 Training Dataset

- **Source**: `TeichAI/claude-4.5-opus-high-reasoning-250x` (250 examples from Claude 4.5 Opus)
- **Format**: Every response contains `<think>` reasoning blocks ‚Äî 100% chain-of-thought
- **Average response length**: ~30,000 chars (median ~5,200)
- **Average think length**: ~2,258 chars (median ~1,089)
- **No data leakage**: Zero exact or substring matches between training prompts and benchmark questions

---

## 3. Overall Scorecard

### 3.1 Full Results

| Benchmark | Metric | n | Distilled | Qwen3-14B | Delta | Direction |
|---|---|---|---|---|---|---|
| AIME 2024 | exact_match | 30 | 17/30 (56.7%) | 21/30 (70.0%) | -13.3pp | üìâ Degraded |
| AIME 2025 | exact_match | 30 | 13/30 (43.3%) | 20/30 (66.7%) | -23.4pp | üìâ Degraded |
| AIME Combined | exact_match | 60 | 30/60 (50.0%) | 41/60 (68.3%) | -18.3pp | üìâ Degraded |
| IFEval | prompt_strict | 541 | 418/541 (77.3%) | 344/541 (63.6%) | +13.7pp | üìà Improved |
| HumanEval+ | pass@1 | 164 | 137/164 (83.5%) | 91/164 (55.5%) | +28.0pp | üìà Improved |

### 3.2 Net Impact

- **Won big**: Coding (+28pp), Instruction following (+13.7pp)
- **Lost significantly**: Competition math (-18.3pp combined)
- **Trade-off**: The distilled model is a much better general-purpose assistant but a worse mathematician

---

## 4. Statistical Significance

All tests use **McNemar's test** ‚Äî the standard test for comparing two classifiers evaluated on the same paired samples. The 2√ó2 contingency table counts questions where both models agreed vs. disagreed.

### 4.1 McNemar's Test Results

| Benchmark | n | Both ‚úÖ | Only Dist ‚úÖ | Only Qwen ‚úÖ | Both ‚ùå | McNemar p-value | Sig | Cohen's h |
|---|---|---|---|---|---|---|---|---|
| **AIME 2024** | 30 | 14 | 3 | 7 | 6 | 0.3438 | ns | h=-0.278 (Small) |
| **AIME 2025** | 30 | 12 | 1 | 8 | 9 | 0.0391 | * | h=-0.474 (Small) |
| **AIME Combined** | 60 | 26 | 4 | 15 | 15 | 0.0192 | * | h=-0.375 (Small) |
| **IFEval** | 541 | 294 | 124 | 50 | 73 | 3.13√ó10‚Åª‚Å∏ | *** | h=+0.302 (Small) |
| **HumanEval+** | 164 | 88 | 49 | 3 | 24 | 4.37√ó10‚Åª¬π‚Å∞ | *** | h=+0.625 (Medium) |

### 4.2 Confidence Intervals (95%, Wilson Score)

| Benchmark | n | Distilled | 95% CI | Qwen3-14B | 95% CI |
|---|---|---|---|---|---|
| AIME 2024 | 30 | 56.7% | [39.2%, 72.6%] | 73.3% | [55.6%, 85.8%] |
| AIME 2025 | 30 | 43.3% | ‚Äî | 66.7% | ‚Äî |
| AIME Combined | 60 | 50.0% | [37.7%, 62.3%] | 68.3% | [55.8%, 78.7%] |
| IFEval | 541 | 77.3% | [73.5%, 80.6%] | 63.6% | [59.4%, 67.5%] |
| HumanEval+ | 164 | 83.5% | [77.1%, 88.4%] | 55.5% | [47.8%, 62.9%] |

### 4.3 Interpretation

| Symbol | Meaning |
|---|---|
| *** | p < 0.001 (highly significant) |
| * | p < 0.05 (significant) |
| ns | p ‚â• 0.05 (not significant) |

- **HumanEval+**: Extremely significant (p < 10‚Åª‚Åπ). The distilled model solved 49 extra problems the base couldn't, and only lost 3. That's a **16:1 ratio** of gains to losses.
- **IFEval**: Extremely significant (p < 10‚Åª‚Å∑). The distilled model passed 124 extra prompts, lost only 50. A **2.5:1 ratio**.
- **AIME 2024 alone**: Not significant (p = 0.34). Only 7 disagreements ‚Äî not enough statistical power with n=30.
- **AIME 2025 alone**: Significant (p = 0.039). 8 questions only Qwen got right vs 1 only distilled got right.
- **AIME Combined**: Significant (p = 0.019). Combining both years doubles the sample size and confirms the math degradation is real.

> AIME 2025 was the decisive addition ‚Äî it doubled the sample size and pushed the math degradation from "maybe just noise" to "confirmed real."

---

## 5. AIME Results: Math Reasoning Degraded

### 5.1 Scores by Year

| Year | n | Distilled | Qwen3-14B | Delta |
|---|---|---|---|---|
| AIME 2024 | 30 | 17/30 (56.7%) | 21/30 (70.0%) | -4 questions |
| AIME 2025 | 30 | 13/30 (43.3%) | 20/30 (66.7%) | -7 questions |
| **Combined** | **60** | **30/60 (50.0%)** | **41/60 (68.3%)** | **-11 questions** |

### 5.2 Agreement Analysis (Combined)

| Category | AIME 2024 | AIME 2025 | Combined |
|---|---|---|---|
| Both correct | 14 | 12 | 26 |
| Both wrong | 6 | 9 | 15 |
| Only Distilled correct | 3 | 1 | 4 |
| Only Qwen3-14B correct | 7 | 8 | 15 |

The pattern is stark: across 60 questions, Qwen3-14B uniquely solved **15 problems** the distilled model missed, while the distilled model uniquely solved only **4**.

### 5.3 AIME 2024 Per-Question Breakdown

| # | Problem ID | Problem (first 60 chars) | Answer | Distilled | Qwen3-14B |
|---|---|---|---|---|---|
| 0 | 2024-II-4 | Let $x,y$ and $z$ be positive real numbers that satisfy the ‚Ä¶ | 33 | ‚úÖ | ‚úÖ |
| 1 | 2024-II-12 | Let $O(0,0), A(\tfrac{1}{2}, 0),$ and $B(0, \tfrac{\sqrt{3}}‚Ä¶ | 23 | ‚úÖ | ‚úÖ |
| 2 | 2024-I-4 | Jen enters a lottery by picking $4$ distinct numbers from $S‚Ä¶ | 116 | ‚úÖ | ‚úÖ |
| 3 | 2024-I-3 | Alice and Bob play the following game. A stack of $n$ tokens‚Ä¶ | 809 | ‚úÖ | ‚úÖ |
| 4 | 2024-I-8 | Eight circles of radius $34$ are sequentially tangent, and t‚Ä¶ | 197 | ‚ùå | ‚ùå |
| 5 | 2024-I-12 | Define $f(x)=\|\| x\|-\tfrac{1}{2}\|$ and $g(x)=\|\| x\|-\tfrac{1}{‚Ä¶ | 385 | ‚ùå | ‚ùå |
| 6 | 2024-I-11 | Each vertex of a regular octagon is independently colored ei‚Ä¶ | 371 | ‚ùå | ‚ùå |
| 7 | 2024-II-11 | Find the number of triples of nonnegative integers $(a,b,c)$‚Ä¶ | 601 | ‚úÖ | ‚úÖ |
| 8 | 2024-I-2 | There exist real numbers $x$ and $y$, both greater than 1, s‚Ä¶ | 25 | ‚úÖ | ‚úÖ |
| 9 | 2024-II-6 | Alice chooses a set $A$ of positive integers. Then Bob lists‚Ä¶ | 55 | ‚úÖ | ‚úÖ |
| 10 | 2024-I-7 | Find the largest possible real part of $(75+117i)z + \frac{‚Ä¶ | 540 | ‚úÖ | ‚úÖ |
| 11 | 2024-II-3 | Find the number of ways to place a digit in each cell of a 2‚Ä¶ | 45 | ‚úÖ | ‚úÖ |
| 12 | 2024-I-1 | Every morning Aya goes for a $9$-kilometer-long walk and sto‚Ä¶ | 204 | ‚úÖ | ‚úÖ |
| 13 | 2024-II-7 | Let $N$ be the greatest four-digit positive integer with the‚Ä¶ | 699 | ‚úÖ | ‚úÖ |
| 14 | 2024-I-6 | Consider the paths of length $16$ that follow the lines from‚Ä¶ | 294 | ‚úÖ | ‚úÖ |
| 15 | 2024-I-13 | Let $p$ be the least prime number for which there exists a p‚Ä¶ | 110 | ‚ùå | ‚úÖ |
| 16 | 2024-I-15 | Let $\mathcal{B}$ be the set of rectangular boxes with surfa‚Ä¶ | 721 | ‚ùå | ‚úÖ |
| 17 | 2024-II-15 | Find the number of rectangles that can be formed inside a fi‚Ä¶ | 315 | ‚ùå | ‚ùå |
| 18 | 2024-II-10 | Let $\triangle ABC$ have circumcenter $O$ and incenter $I$ w‚Ä¶ | 468 | ‚úÖ | ‚ùå |
| 19 | 2024-II-9 | There is a collection of $25$ indistinguishable white chips ‚Ä¶ | 902 | ‚ùå | ‚ùå |
| 20 | 2024-II-14 | Let $b \geq 2$ be an integer. Call a positive integer $n$ $b‚Ä¶ | 211 | ‚ùå | ‚úÖ |
| 21 | 2024-II-5 | Let ABCDEF be a convex equilateral hexagon in which all pair‚Ä¶ | 80 | ‚ùå | ‚úÖ |
| 22 | 2024-I-9 | Let $A$, $B$, $C$, and $D$ be points on the hyperbola $\frac‚Ä¶ | 480 | ‚úÖ | ‚úÖ |
| 23 | 2024-II-2 | A list of positive integers has the following properties: \b‚Ä¶ | 236 | ‚ùå | ‚úÖ |
| 24 | 2024-II-1 | Among the 900 residents of Aimeville, there are 195 who own ‚Ä¶ | 73 | ‚úÖ | ‚úÖ |
| 25 | 2024-I-10 | Let $ABC$ be a triangle inscribed in circle $\omega$. Let th‚Ä¶ | 113 | ‚úÖ | ‚úÖ |
| 26 | 2024-II-8 | Torus $T$ is the surface produced by revolving a circle with‚Ä¶ | 127 | ‚ùå | ‚ùå |
| 27 | 2024-I-14 | Let $ABCD$ be a tetrahedron such that $AB=CD= \sqrt{41}$, $A‚Ä¶ | 104 | ‚ùå | ‚ùå |
| 28 | 2024-I-5 | Rectangles $ABCD$ and $EFGH$ are drawn such that $D,E,C,F$ a‚Ä¶ | 104 | ‚ùå | ‚úÖ |
| 29 | 2024-II-13 | Let $\omega \neq 1$ be a 13th root of unity. Find the remain‚Ä¶ | 321 | ‚úÖ | ‚úÖ |

### 5.4 AIME 2025 Per-Question Breakdown

| # | Problem (first 60 chars) | Answer | Distilled | Qwen3-14B | Qwen Think (chars) |
|---|---|---|---|---|---|
| 0 | Find the sum of all integer bases $b>9$ for which $17_b$ is ‚Ä¶ | 70 | ‚úÖ | ‚úÖ | 9,909 |
| 1 | In $\triangle ABC$ points $D$ and $E$ lie on $\overline{AB}$‚Ä¶ | 588 | ‚ùå | ‚úÖ | 39,006 |
| 2 | The $9$ members of a baseball team went to an ice-cream parl‚Ä¶ | 16 | ‚úÖ | ‚úÖ | 22,178 |
| 3 | Find the number of ordered pairs $(x,y)$, where both $x$ and‚Ä¶ | 117 | ‚úÖ | ‚úÖ | 25,820 |
| 4 | There are $8!= 40320$ eight-digit positive integers that use‚Ä¶ | 279 | ‚ùå | ‚úÖ | 39,420 |
| 5 | An isosceles trapezoid has an inscribed circle tangent to ea‚Ä¶ | 504 | ‚úÖ | ‚úÖ | 12,953 |
| 6 | The twelve letters $A$,$B$,$C$,$D$,$E$,$F$,$G$,$H$,$I$,$J$,$‚Ä¶ | 821 | ‚ùå | ‚ùå | 70,084 |
| 7 | Let $k$ be a real number such that the system‚Ä¶ | 77 | ‚ùå | ‚úÖ | 9,320 |
| 8 | The parabola with equation $y = x^2 - 4$ is rotated $60^\cir‚Ä¶ | 62 | ‚úÖ | ‚úÖ | 53,688 |
| 9 | The $27$ cells of a $3 \times 9$ grid are filled in using th‚Ä¶ | 81 | ‚ùå | ‚ùå | 101,619 |
| 10 | A piecewise linear function is defined by‚Ä¶ | 259 | ‚ùå | ‚ùå | 72,245 |
| 11 | The set of points in $3$-dimensional coordinate space that l‚Ä¶ | 510 | ‚ùå | ‚úÖ | 67,142 |
| 12 | Alex divides a disk into four quadrants with two perpendicul‚Ä¶ | 204 | ‚ùå | ‚ùå | 82,099 |
| 13 | Let $ABCDE$ be a convex pentagon with $AB=14,$ $BC=7,$ $CD=2‚Ä¶ | 60 | ‚ùå | ‚ùå | 75,397 |
| 14 | Let $N$ denote the number of ordered triples of positive int‚Ä¶ | 735 | ‚ùå | ‚ùå | 64,221 |
| 15 | Six points $A, B, C, D, E,$ and $F$ lie in a straight line i‚Ä¶ | 468 | ‚úÖ | ‚úÖ | 13,666 |
| 16 | Find the sum of all positive integers $n$ such that $n + 2$ ‚Ä¶ | 49 | ‚úÖ | ‚úÖ | 10,821 |
| 17 | Four unit squares form a $2 \times 2$ grid. Each of the $12$‚Ä¶ | 82 | ‚ùå | ‚úÖ | 47,158 |
| 18 | The product $\prod^{63}_{k=4}$‚Ä¶ | 106 | ‚ùå | ‚úÖ | 24,139 |
| 19 | Suppose $\triangle ABC$ has angles $\angle BAC = 84^\circ, \‚Ä¶ | 336 | ‚ùå | ‚úÖ | 44,901 |
| 20 | Circle $\omega_1$ with radius $6$ centered at point $A$ is i‚Ä¶ | 293 | ‚úÖ | ‚úÖ | 26,545 |
| 21 | Let $A$ be the set of positive integer divisors of $2025$. L‚Ä¶ | 237 | ‚úÖ | ‚úÖ | 29,676 |
| 22 | From an unlimited supply of 1-cent coins, 10-cent coins, and‚Ä¶ | 610 | ‚ùå | ‚ùå | 61,796 |
| 23 | There are $n$ values of $x$ in the interval $0<x<2\pi$ where‚Ä¶ | 149 | ‚úÖ | ‚ùå | 23,315 |
| 24 | Sixteen chairs are arranged in a row. Eight people each sele‚Ä¶ | 907 | ‚úÖ | ‚úÖ | 35,617 |
| 25 | Let $S$ be the set of vertices of a regular $24$-gon. Find t‚Ä¶ | 113 | ‚ùå | ‚úÖ | 57,779 |
| 26 | Let $A_1A_2\dots A_{11}$ be a non-convex $11$-gon such that ‚Ä¶ | 19 | ‚úÖ | ‚úÖ | 21,235 |
| 27 | Let the sequence of rationals $x_1,x_2,\dots$ be defined suc‚Ä¶ | 248 | ‚ùå | ‚ùå | 71,740 |
| 28 | Let ${\triangle ABC}$ be a right triangle with $\angle A = 9‚Ä¶ | 104 | ‚úÖ | ‚úÖ | 30,722 |
| 29 | Let $f(x)=\frac{(x-18)(x-72)(x-98)(x-k)}{x}$‚Ä¶ | 240 | ‚ùå | ‚ùå | 73,596 |

### 5.5 Accuracy by Math Topic (Combined AIME)

| Topic | AIME 2024 Dist | AIME 2024 Qwen | AIME 2025 Dist | AIME 2025 Qwen |
|---|---|---|---|---|
| Number Theory | 10/18 (55.6%) | 14/18 (77.8%) | 10/21 (47.6%) | 14/21 (66.7%) |
| Geometry | 5/12 (41.7%) | 7/12 (58.3%) | 7/15 (46.7%) | 11/15 (73.3%) |
| Combinatorics | 2/3 (66.7%) | 2/3 (66.7%) | 3/9 (33.3%) | 4/9 (44.4%) |
| Algebra | 3/4 (75.0%) | 4/4 (100.0%) | 1/4 (25.0%) | 3/4 (75.0%) |
| Probability | 1/2 (50.0%) | 1/2 (50.0%) | 1/4 (25.0%) | 1/4 (25.0%) |

The base Qwen3-14B consistently outperforms on **number theory** and **geometry** ‚Äî the hardest AIME sub-categories requiring deep multi-step reasoning chains.

---

## 6. IFEval Results: Instruction Following Improved

### 6.1 Prompt-Level Agreement

| Category | Count | % |
|---|---|---|
| Both pass | 294 | 54.3% |
| Both fail | 73 | 13.5% |
| Only Distilled pass | 124 | 22.9% |
| Only Qwen3-14B pass | 50 | 9.2% |
| **Total** | **541** | |

The distilled model uniquely passed **124 prompts** vs only **50** for the base ‚Äî a 2.5:1 ratio.

### 6.2 Performance by Instruction Category

| Instruction Category | n | Distilled Acc | Qwen3-14B Acc | Delta |
|---|---|---|---|---|
| detectable_content | 53 | 96.2% | 64.2% | **+32.1pp** |
| startend | 67 | 88.1% | 61.2% | **+26.9pp** |
| length_constraints | 143 | 75.5% | 50.3% | **+25.2pp** |
| language | 31 | 96.8% | 80.6% | +16.1pp |
| keywords | 163 | 84.7% | 72.4% | +12.3pp |
| detectable_format | 157 | 91.7% | 79.0% | +12.7pp |
| punctuation | 66 | 97.0% | 87.9% | +9.1pp |
| change_case | 89 | 75.3% | 73.0% | +2.2pp |
| combination | 65 | 61.5% | 69.2% | **-7.7pp** |

The distilled model improved on **8 of 9** instruction categories. The only loss was `combination` (repeat_prompt), where the distilled model's Claude-style verbose responses interfered with exact prompt repetition.

### 6.3 Biggest Wins by Specific Instruction Type

| Instruction ID | n | Distilled | Qwen3-14B | Advantage |
|---|---|---|---|---|
| detectable_format:multiple_sections | 14 | 92.9% | 42.9% | **+50.0pp** |
| combination:two_responses | 24 | 100.0% | 62.5% | **+37.5pp** |
| detectable_content:postscript | 26 | 96.2% | 61.5% | **+34.6pp** |
| length_constraints:number_words | 52 | 67.3% | 36.5% | **+30.8pp** |
| detectable_content:number_placeholders | 27 | 96.3% | 66.7% | **+29.6pp** |
| detectable_format:json_format | 17 | 82.4% | 52.9% | **+29.4pp** |

### 6.4 Statistical Significance by Category

| Category | n | p-value | Sig |
|---|---|---|---|
| detectable_content | 53 | 1.53√ó10‚Åª‚Åµ | *** |
| startend | 67 | 0.0001 | *** |
| length_constraints | 143 | 4.31√ó10‚Åª‚Å∂ | *** |
| keywords | 163 | 0.0008 | *** |
| detectable_format | 157 | 0.0005 | *** |
| punctuation | 66 | 0.0703 | ns |
| change_case | 89 | 0.8501 | ns |
| combination | 65 | 0.5218 | ns |
| language | 31 | 0.1250 | ns |

---

## 7. HumanEval+ Results: Coding Dramatically Improved

### 7.1 Agreement Analysis

| Category | Count | % |
|---|---|---|
| Both pass | 88 | 53.7% |
| Both fail | 24 | 14.6% |
| Only Distilled pass | **49** | **29.9%** |
| Only Qwen3-14B pass | 3 | 1.8% |
| **Total** | **164** | |

This is the most lopsided result across all benchmarks: a **16:1 ratio** of gains to losses. The distilled model solved 49 extra problems while only losing 3.

### 7.2 Performance by Problem Category

| Category | n | Distilled Acc | Qwen3-14B Acc | Delta |
|---|---|---|---|---|
| List Operations | 91 | 91.2% | 52.7% | **+38.5pp** |
| Sorting/Searching | 46 | 87.0% | 50.0% | **+37.0pp** |
| Data Structure | 17 | 82.4% | 47.1% | **+35.3pp** |
| Math/Number | 114 | 83.3% | 51.8% | **+31.6pp** |
| Logic/Control | 77 | 79.2% | 54.5% | **+24.7pp** |
| String Manipulation | 69 | 82.6% | 63.8% | **+18.8pp** |

Improvements were **broad-based** across all coding categories ‚Äî the distilled model didn't just learn specific tricks but developed generally better code generation.

### 7.3 Problems Only Distilled Solved (49 total)

`filter_by_substring`, `rolling_max`, `string_xor`, `find_closest_elements`, `filter_integers`, `largest_divisor`, `factorize`, `filter_by_prefix`, `unique`, `triples_sum_to_zero`, `pairs_sum_to_zero`, `fib4`, `median`, `modp`, `decode_shift`, `below_threshold`, `common`, `largest_prime_factor`, `derivative`, `triangle_area`, `starts_one_ends`, `get_row`, `sort_array`, `encode`, `skjkasdkd`, `words_string`, `rounded_avg`, `count_nums`, `move_one_ball`, `minSubArraySum`, `max_fill`, `select_words`, `get_closest_vowel`, `match_parens`, `is_sorted`, `intersection`, `prod_signs`, `tri`, `digits`, `sum_squares`, `file_name_check`, `specialFilter`, `double_the_difference`, `compare`, `Strongest_Extension`, `even_odd_count`, `find_max`, `eat`, `generate_integers`

### 7.4 Problems Only Qwen3-14B Solved (3 total)

`vowels_count`, `any_int`, `multiply`

---

## 8. Reasoning Volume Analysis (Proxy Logs)

All API requests were routed through logging proxies, capturing the full request and response including `<think>` reasoning blocks. This provides unique insight into **how much each model reasons** before answering.

### 8.1 Data Sources & Match Quality

| File | Total Entries | AIME | IFEval | HumanEval | Other |
|---|---|---|---|---|---|
| `proxy_log.jsonl` | 345 | 42 | 46 | 251 | 6 |
| `proxy_log_qwen3.jsonl` | 820 | 30 | 514 | 242 | 34 |
| `proxy_log_ex1.jsonl` | 841 | 91 | 46 | 688 | 16 |

| Run | Match Rate | Source | Note |
|---|---|---|---|
| Distilled AIME 2024 | 100% (30/30) | proxy_log.jsonl | Full proxy data |
| Distilled AIME 2025 | 3.3% (1/30) | proxy_log_ex1.jsonl | Mostly cached ‚Äî fallback to sample JSONL |
| Qwen AIME 2024 | 100% (30/30) | proxy_log_ex1.jsonl | Full proxy data |
| Qwen AIME 2025 | 100% (30/30) | proxy_log_ex1.jsonl | Full proxy data |
| Distilled IFEval | 4.6% (25/541) | proxy_log.jsonl | Mostly cached ‚Äî fallback to sample JSONL |
| Qwen IFEval | 95.0% (514/541) | proxy_log_qwen3.jsonl | Full proxy data |
| Distilled HumanEval+ | 98.2% (161/164) | proxy_log.jsonl | Full proxy data |
| Qwen HumanEval+ | 98.2% (161/164) | proxy_log_qwen3.jsonl | Full proxy data |

### 8.2 Reasoning Volume by Model and Benchmark

| Model | Benchmark | n | Avg Think (chars) | Avg Completion (chars) | Think Ratio | % with Think | Avg Time (s) |
|---|---|---|---|---|---|---|---|
| **Distilled** | **AIME 2024** | 30 | **13,742** | 2,548 | 84% | 100.0% | 187.2s |
| **Qwen3-14B** | **AIME 2024** | 30 | **36,743** | 2,358 | 94% | 100.0% | 394.3s |
| **Distilled** | **AIME 2025** | 30 | **773** | 2,168 | 26% | 3.3% | 279.1s |
| **Qwen3-14B** | **AIME 2025** | 30 | **43,927** | 2,626 | 94% | 100.0% | 460.2s |
| **Distilled** | **IFEval** | 541 | 49 | 2,061 | 2% | 4.6% | 573.0s |
| **Qwen3-14B** | **IFEval** | 541 | 1,687 | 743 | 69% | 95.0% | 69.4s |
| **Distilled** | **HumanEval+** | 164 | 1,234 | 836 | 60% | 45.7% | 196.1s |
| **Qwen3-14B** | **HumanEval+** | 164 | 0 | 359 | 0% | 0.0% | 102.7s |

### 8.3 The Reasoning Gap on AIME

| Metric | Distilled | Qwen3-14B | Ratio |
|---|---|---|---|
| AIME 2024 avg think chars | 13,742 | 36,743 | **0.37√ó** (Distilled reasons 2.7√ó less) |
| AIME 2025 avg think chars | 773 | 43,927 | **0.02√ó** (Distilled reasons 57√ó less) |

The distilled model's reasoning chains are **dramatically shorter** on AIME problems. On AIME 2024, where we have full proxy data for both models, the distilled model produces less than 40% of the reasoning the base model does. On AIME 2025, the gap is even more extreme (most distilled responses were served from cache, so only 1 had proxy data).

### 8.4 Reasoning Length vs Correctness

Does more reasoning correlate with getting the right answer?

| Model | Benchmark | Correct: Avg Think | Incorrect: Avg Think | Œî |
|---|---|---|---|---|
| Distilled | AIME 2024 | 11,779 | 16,310 | -4,531 |
| Qwen3-14B | AIME 2024 | **24,383** | **65,582** | **-41,198** |
| Qwen3-14B | AIME 2025 | **31,085** | **69,611** | **-38,526** |
| Distilled | HumanEval+ | 907 | 2,893 | -1,986 |
| Qwen3-14B | IFEval | 1,433 | 2,131 | -698 |

**Key insight**: For **both** models, **incorrect answers involve MORE reasoning** ‚Äî the models reason longer when they're struggling. But the base Qwen3-14B's longer reasoning chains still produce more correct answers overall on AIME, suggesting that its extended reasoning is more often productive.

For Qwen on AIME 2024: correct answers average 24K chars of thinking, but wrong answers average **65K chars** ‚Äî the model spends almost 3√ó more time thinking on problems it gets wrong.

### 8.5 Distillation Changed Reasoning Patterns

| Behavior | Distilled | Base Qwen3-14B |
|---|---|---|
| Thinks on AIME | Yes (shorter chains) | Yes (much longer chains) |
| Thinks on IFEval | Almost never (4.6%) | Almost always (95%) |
| Thinks on HumanEval+ | Sometimes (45.7%) | Never (0%) |
| Completion length on IFEval | Long (2,061 chars) | Short (743 chars) |
| Completion length on HumanEval+ | Long (836 chars) | Short (359 chars) |

The distillation fundamentally changed the model's reasoning allocation:
- **Compressed** math reasoning (shorter `<think>` blocks on AIME)
- **Eliminated** reasoning on IFEval (replaced with longer, more thorough completions)
- **Added** reasoning to coding (45.7% of HumanEval+ responses now have `<think>` blocks, vs 0% for base)
- **Produced** longer, more detailed final answers across the board

---

## 9. Training Dataset Composition

### 9.1 Overview

| Property | Value |
|---|---|
| Source | `TeichAI/claude-4.5-opus-high-reasoning-250x` |
| Total examples | 250 |
| Format | Chat messages (system / user / assistant) |
| Thinking tags | 100% contain `<think>` blocks |
| Avg prompt length | 136 chars |
| Avg response length | ~30,072 chars |
| Avg think length | ~2,258 chars (median ~1,089) |
| Data leakage | **None** (0 matches with any benchmark) |

### 9.2 Topic Distribution

| Category | Count | % | Benchmark Relevance |
|---|---|---|---|
| Math (simple) | 105 | 42% | Weak ‚Üí AIME (simple arithmetic vs competition-level) |
| Coding | 43 | 17% | Partial ‚Üí HumanEval+ (high-level vs function completion) |
| General/Other | 28 | 11% | Weak ‚Üí IFEval |
| Requirements Engineering | 18 | 7% | None |
| Finance/Business | 17 | 7% | None |
| Legal | 13 | 5% | None |
| Science | 11 | 4% | None |
| Philosophy | 8 | 3% | None |
| Medical/Health | 7 | 3% | None |

### 9.3 The Difficulty Gap

Even where topics overlap, the **difficulty levels are vastly different**:

| Skill | Training Dataset | Benchmark |
|---|---|---|
| **Math** | Simple arithmetic ("What is 3‚Å¥ minus 5¬≤?", "Solve 3x + 15 = 6(x-2)") | AIME competition math (number theory, complex polynomials) |
| **Coding** | High-level project descriptions ("Build a recommendation system") | Precise function signatures with test cases (`def factorize(n)`) |
| **Instruction Following** | Open-ended requests ("Explain...", "Summarize...") | Rigid format constraints ("all caps", "exactly N words", "no commas") |

### 9.4 Think Length Distribution in Training Data

| Think Length Bucket | Count | % |
|---|---|---|
| 1‚Äì500 chars | 48 | 19.2% |
| 501‚Äì2,000 chars | 115 | 46.0% |
| 2,001‚Äì5,000 chars | 63 | 25.2% |
| 5,001‚Äì20,000 chars | 23 | 9.2% |
| >20,000 chars | 1 | 0.4% |

The training data has **moderate** reasoning ‚Äî median ~1,089 chars. The base Qwen3-14B naturally reasons **much more** on hard problems (36K‚Äì44K chars on AIME), so the distillation effectively **compressed** the model's reasoning patterns to match the shorter training examples.

---

## 10. Dataset vs Performance: What Drove the Results

### 10.1 Skill Transfer Quadrant Analysis

We categorized every skill by: (a) how many training examples covered it, and (b) whether the distilled model improved on benchmark questions requiring that skill.

```
                    ‚îÇ Positive Œî (Distilled Better)
                    ‚îÇ
  Q1: Direct        ‚îÇ  Q2: Generalization /
  Transfer          ‚îÇ  Transfer Learning
  (High Rep +       ‚îÇ  (Low Rep +
   Improved)        ‚îÇ   Improved)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Q3: Didn't Help   ‚îÇ  Q4: Expected
  (High Rep +       ‚îÇ  Weakness
   No Gain/Worse)   ‚îÇ  (Low Rep +
                    ‚îÇ   No Gain/Worse)
                    ‚îÇ
  High Dataset Rep  ‚îÇ  Low Dataset Rep
```

| Quadrant | Description | Count | % |
|---|---|---|---|
| **Q1** | Direct Transfer (High Rep + Improved) | 17 | **41%** |
| **Q2** | Generalization (Low Rep + Improved) | 11 | **27%** |
| **Q3** | Didn't Help (High Rep + No Gain) | 9 | 22% |
| **Q4** | Expected Weakness (Low Rep + No Gain) | 4 | 10% |

**68% of skill-benchmark pairs** showed improvement (Q1 + Q2), and **27% improved despite minimal training data** ‚Äî strong evidence of generalization from chain-of-thought distillation.

### 10.2 Correlation: Dataset Representation vs Improvement

| Benchmark | Spearman œÅ | p-value | Interpretation |
|---|---|---|---|
| AIME 2024 | 0.017 | 0.97 | No correlation |
| IFEval | 0.055 | 0.82 | No correlation |
| HumanEval+ | 0.219 | 0.41 | Weak positive (not significant) |

**The number of training examples for a skill does NOT predict improvement.** This confirms that the gains come from **general chain-of-thought transfer**, not skill-specific training.

### 10.3 Chain-of-Thought vs Direct Skill Transfer

If CoT training (not skill-specific exposure) drives improvement, we'd expect uniform improvement regardless of representation:

| Benchmark | High-rep skills mean Œî | Low-rep skills mean Œî | Gap | Interpretation |
|---|---|---|---|---|
| AIME 2024 | -12.9pp | -25.0pp | +12.1pp | Some skill transfer effect |
| IFEval | +10.7pp | +4.9pp | +5.8pp | Mostly uniform (CoT-driven) |
| HumanEval+ | +34.5pp | +30.1pp | +4.4pp | **Uniform** ‚Äî CoT-driven |

HumanEval+ shows the clearest CoT-driven pattern: even skills with almost no training examples (Function Completion: 2 examples ‚Üí +28pp on 164 questions) saw massive improvement.

### 10.4 What the Dataset Actually Taught

| What Changed | Why | Dataset Role |
|---|---|---|
| **HumanEval+ +28pp** | Systematic coding approach ‚Üí more correct implementations | CoT patterns + some coding examples |
| **IFEval +13.7pp** | Structured reasoning ‚Üí better format adherence | Indirect: CoT patterns transfer to instruction following |
| **AIME -18.3pp** | Compressed reasoning chains insufficient for competition math | **Harmful**: Simple math at wrong difficulty level |

---

## 11. Comparison with Other Models

For context, here's how both models compare to other systems on the same benchmarks. Scores for other models are sourced from [llm-stats.com](https://llm-stats.com) and [EvalPlus](https://evalplus.github.io/leaderboard.html).

> **Note:** Our models are **14B parameter** models run locally on consumer hardware. "‚Äî" = no data available.

| Model | Params | AIME 2024 | AIME 2025 | IFEval | HumanEval+ |
|---|---|---|---|---|---|
| **Distilled (ours)** | **14B** | **56.7%** | **43.3%** | **77.3%** | **83.5%** |
| **Qwen3-14B (base)** | **14B** | **70.0%** | **66.7%** | **63.6%** | **55.5%** |
| Phi 4 Reasoning Plus | 14B | 81.3% | 78.0% | 84.9% | ‚Äî |
| Phi 4 Reasoning | 14B | 75.3% | 62.9% | 83.4% | ‚Äî |
| Claude 3.7 Sonnet | ‚Äî | 80.0% | 54.8% | 93.2% | ‚Äî |
| GPT-4.1 | ‚Äî | 48.1% | 46.4% | 87.4% | ‚Äî |
| GPT-4.1 mini | ‚Äî | 49.6% | 40.2% | 84.1% | ‚Äî |
| Kimi K2 Instruct | 1T (MoE) | 69.6% | 49.5% | 89.8% | ‚Äî |
| DeepSeek-V3 | 671B (MoE) | 39.2% | ‚Äî | 86.1% | 86.6% |
| O1 Preview | ‚Äî | ‚Äî | ‚Äî | ‚Äî | 89.0% |
| GPT-4o | ‚Äî | ‚Äî | ‚Äî | 81.0% | 87.2% |

### Notable Comparisons

- **HumanEval+ 83.5%** puts the distilled 14B model close to DeepSeek-V3 (86.6%) and GPT-4o (87.2%) ‚Äî models that are 10-50√ó larger
- **IFEval 77.3%** surpasses GPT-4.1 mini (84.1% uses different metric) and is competitive for a 14B model
- **AIME**: Both Phi 4 Reasoning models at 14B significantly outperform ‚Äî dedicated math reasoning training produces better results than general CoT distillation

---

## 12. Conclusions

### 12.1 What We Learned

1. **Chain-of-thought distillation from a frontier model provides broad capability uplift with only 250 examples.** The distilled model gained +28pp on coding and +13.7pp on instruction following ‚Äî remarkable for such a tiny training set.

2. **The distillation transferred "how to think," not "what to think about."** Improvement was uniform across skill types regardless of dataset representation (Spearman œÅ ‚âà 0), and skills with almost no training examples still saw massive gains (e.g., Function Completion: 2 training examples ‚Üí +28pp improvement on 164 questions).

3. **Competition math suffered because reasoning chains were compressed.** The base Qwen3-14B produces 37K‚Äì44K chars of reasoning on AIME problems; the distilled model produces only 0.8K‚Äì14K. The training data's moderate reasoning length (~2,258 chars average) appears to have set a ceiling on how much the model reasons, and this ceiling is too low for competition math.

4. **More reasoning doesn't always mean better answers ‚Äî but less reasoning is worse for hard problems.** Both models reason longer on wrong answers (struggling), but the base model's willingness to reason at length gives it more chances to find the right path on hard AIME problems.

5. **The difficulty gap matters more than topic overlap.** 42% of training examples were math, but at arithmetic level ‚Äî not competition level. The model learned math reasoning patterns but not the depth required for AIME.

### 12.2 The Trade-Off

| Dimension | Effect |
|---|---|
| **Coding** | üöÄ Massive improvement (+28pp) |
| **Instruction Following** | ‚úÖ Strong improvement (+13.7pp) |
| **Response Quality** | ‚úÖ Longer, more thorough completions (Claude-style) |
| **Reasoning Flexibility** | ‚úÖ Added `<think>` to coding (45.7%), removed from IFEval |
| **Competition Math** | üìâ Significant degradation (-18.3pp) |
| **Reasoning Depth** | üìâ Compressed chains insufficient for hardest problems |

### 12.3 Practical Implications

- **For general-purpose use**: The distilled model is significantly better ‚Äî it codes better, follows instructions better, and produces more thorough responses.
- **For math-heavy tasks**: Use the base Qwen3-14B or a dedicated math reasoning model like Phi 4 Reasoning.
- **For future distillation**: Include competition-level math examples at appropriate difficulty to avoid regression. The reasoning compression effect suggests training data reasoning length may set an implicit ceiling.
- **For the field**: Even 250 high-quality CoT examples from a frontier model can produce remarkable capability transfer, but domain-specific difficulty matching is essential to avoid regression in hard reasoning tasks.

---

*Report generated from evaluation data collected February 21, 2026. Models served via llama.cpp on consumer hardware. All statistical tests use McNemar's paired test with Wilson confidence intervals.*
